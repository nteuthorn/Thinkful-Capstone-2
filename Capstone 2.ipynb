{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import itertools\n",
    "import scipy.stats as stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import 2015 and 2017 census data, change column names to be consistent across datasets\n",
    "\n",
    "df2015 = pd.read_csv('acs2015_census_tract_data.csv')\n",
    "\n",
    "df2017 = pd.read_csv('acs2017_census_tract_data.csv')\n",
    "df2017.rename({'TractId': 'CensusTract', 'VotingAgeCitizen': 'Citizen'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change count statistics to percentages based on total population; remove %women because of perfect multicollinearity with %men\n",
    "\n",
    "df2015['Men'] = df2015['Men']/df2015['TotalPop']\n",
    "df2015['Citizen'] = df2015['Citizen']/df2015['TotalPop']\n",
    "df2015['Employed'] = df2015['Employed']/df2015['TotalPop']\n",
    "df2015.drop(['CensusTract', 'Women'], axis=1, inplace=True)\n",
    "\n",
    "df2017['Men'] = df2017['Men']/df2017['TotalPop']\n",
    "df2017['Citizen'] = df2017['Citizen']/df2017['TotalPop']\n",
    "df2017['Employed'] = df2017['Employed']/df2017['TotalPop']\n",
    "df2017.drop(['CensusTract', 'Women'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df_diff as change from 2015 to 2017\n",
    "#Add Poverty_Change variable as categorical, -1=significant decrease in poverty, 0=no significant change, +1=significant increase in poverty\n",
    "\n",
    "df_diff = df2017.copy()\n",
    "df_diff.drop(['State', 'County'], axis=1, inplace=True)\n",
    "for col in df_diff:\n",
    "        df_diff[col] = df2017[col] - df2015[col]\n",
    "df_diff['Poverty_Change'] = 0\n",
    "df_diff.loc[(df_diff['Poverty'] >= 1.5), 'Poverty_Change'] = 1\n",
    "df_diff.loc[(df_diff['Poverty'] <= -1.5), 'Poverty_Change'] = -1\n",
    "\n",
    "#Add poverty change to 2015 dataset\n",
    "df2015['Poverty_Change'] = df_diff['Poverty_Change']\n",
    "df2015['Poverty_Change_val'] = df_diff['Poverty']\n",
    "\n",
    "df2015['Poverty_Change_2c'] = 0\n",
    "df2015.loc[(df2015['Poverty_Change_val'] < 0), 'Poverty_Change_2c'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PCA components to capture 50% of the variation in 'State' and 'County' variables\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(pd.concat([pd.get_dummies(df2015['State']), pd.get_dummies(df2015['County'])], axis=1))\n",
    "\n",
    "principalDf = pd.DataFrame(data = principal_components)\n",
    "df2015 = pd.concat([df2015, principalDf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null values\n",
    "\n",
    "df2015.dropna(inplace=True)\n",
    "df_diff.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalPop\n",
      "There are 66 tracts identified as outliers.\n",
      "Men\n",
      "There are 160 tracts identified as outliers.\n",
      "Hispanic\n",
      "There are 0 tracts identified as outliers.\n",
      "White\n",
      "There are 0 tracts identified as outliers.\n",
      "Black\n",
      "There are 2367 tracts identified as outliers.\n",
      "Native\n",
      "There are 3223 tracts identified as outliers.\n",
      "Asian\n",
      "There are 2205 tracts identified as outliers.\n",
      "Pacific\n",
      "There are 8847 tracts identified as outliers.\n",
      "Citizen\n",
      "There are 2 tracts identified as outliers.\n",
      "Income\n",
      "There are 22 tracts identified as outliers.\n",
      "IncomeErr\n",
      "There are 223 tracts identified as outliers.\n",
      "IncomePerCap\n",
      "There are 221 tracts identified as outliers.\n",
      "IncomePerCapErr\n",
      "There are 688 tracts identified as outliers.\n",
      "Poverty\n",
      "There are 0 tracts identified as outliers.\n",
      "ChildPoverty\n",
      "There are 0 tracts identified as outliers.\n",
      "Professional\n",
      "There are 0 tracts identified as outliers.\n",
      "Service\n",
      "There are 1 tracts identified as outliers.\n",
      "Office\n",
      "There are 1 tracts identified as outliers.\n",
      "Construction\n",
      "There are 35 tracts identified as outliers.\n",
      "Production\n",
      "There are 0 tracts identified as outliers.\n",
      "Drive\n",
      "There are 345 tracts identified as outliers.\n",
      "Carpool\n",
      "There are 19 tracts identified as outliers.\n",
      "Transit\n",
      "There are 3709 tracts identified as outliers.\n",
      "Walk\n",
      "There are 1545 tracts identified as outliers.\n",
      "OtherTransp\n",
      "There are 501 tracts identified as outliers.\n",
      "WorkAtHome\n",
      "There are 89 tracts identified as outliers.\n",
      "MeanCommute\n",
      "There are 0 tracts identified as outliers.\n",
      "Employed\n",
      "There are 0 tracts identified as outliers.\n",
      "PrivateWork\n",
      "There are 16 tracts identified as outliers.\n",
      "PublicWork\n",
      "There are 76 tracts identified as outliers.\n",
      "SelfEmployed\n",
      "There are 25 tracts identified as outliers.\n",
      "FamilyWork\n",
      "There are 16564 tracts identified as outliers.\n",
      "Unemployment\n",
      "There are 73 tracts identified as outliers.\n",
      "Poverty_Change\n",
      "There are 0 tracts identified as outliers.\n",
      "Poverty_Change_val\n",
      "There are 22 tracts identified as outliers.\n",
      "Poverty_Change_2c\n",
      "There are 0 tracts identified as outliers.\n",
      "0\n",
      "There are 7933 tracts identified as outliers.\n",
      "1\n",
      "There are 9985 tracts identified as outliers.\n",
      "2\n",
      "There are 14089 tracts identified as outliers.\n",
      "3\n",
      "There are 4109 tracts identified as outliers.\n",
      "4\n",
      "There are 3106 tracts identified as outliers.\n",
      "5\n",
      "There are 3209 tracts identified as outliers.\n",
      "6\n",
      "There are 5781 tracts identified as outliers.\n",
      "7\n",
      "There are 0 tracts identified as outliers.\n",
      "8\n",
      "There are 4021 tracts identified as outliers.\n",
      "9\n",
      "There are 6109 tracts identified as outliers.\n",
      "TotalPop\n",
      "There are 887 tracts identified as outliers.\n",
      "Men\n",
      "There are 83 tracts identified as outliers.\n",
      "Hispanic\n",
      "There are 1336 tracts identified as outliers.\n",
      "White\n",
      "There are 290 tracts identified as outliers.\n",
      "Black\n",
      "There are 3661 tracts identified as outliers.\n",
      "Native\n",
      "There are 32067 tracts identified as outliers.\n",
      "Asian\n",
      "There are 4721 tracts identified as outliers.\n",
      "Pacific\n",
      "There are 10419 tracts identified as outliers.\n",
      "Citizen\n",
      "There are 163 tracts identified as outliers.\n",
      "Income\n",
      "There are 655 tracts identified as outliers.\n",
      "IncomeErr\n",
      "There are 765 tracts identified as outliers.\n",
      "IncomePerCap\n",
      "There are 964 tracts identified as outliers.\n",
      "IncomePerCapErr\n",
      "There are 2951 tracts identified as outliers.\n",
      "Poverty\n",
      "There are 340 tracts identified as outliers.\n",
      "ChildPoverty\n",
      "There are 717 tracts identified as outliers.\n",
      "Professional\n",
      "There are 69 tracts identified as outliers.\n",
      "Service\n",
      "There are 169 tracts identified as outliers.\n",
      "Office\n",
      "There are 83 tracts identified as outliers.\n",
      "Construction\n",
      "There are 305 tracts identified as outliers.\n",
      "Production\n",
      "There are 267 tracts identified as outliers.\n",
      "Drive\n",
      "There are 147 tracts identified as outliers.\n",
      "Carpool\n",
      "There are 269 tracts identified as outliers.\n",
      "Transit\n",
      "There are 7794 tracts identified as outliers.\n",
      "Walk\n",
      "There are 3191 tracts identified as outliers.\n",
      "OtherTransp\n",
      "There are 2348 tracts identified as outliers.\n",
      "WorkAtHome\n",
      "There are 558 tracts identified as outliers.\n",
      "MeanCommute\n",
      "There are 255 tracts identified as outliers.\n",
      "Employed\n",
      "There are 52 tracts identified as outliers.\n",
      "PrivateWork\n",
      "There are 124 tracts identified as outliers.\n",
      "PublicWork\n",
      "There are 179 tracts identified as outliers.\n",
      "SelfEmployed\n",
      "There are 270 tracts identified as outliers.\n",
      "FamilyWork\n",
      "There are 18954 tracts identified as outliers.\n",
      "Unemployment\n",
      "There are 395 tracts identified as outliers.\n",
      "Poverty_Change\n",
      "There are 0 tracts identified as outliers.\n"
     ]
    }
   ],
   "source": [
    "#Winsorize outliers\n",
    "\n",
    "for col in df2015.columns[2:]:\n",
    "    threshold = 5\n",
    "    q75, q25 = np.percentile(df2015[col], [75,25])\n",
    "    iqr = q75-q25\n",
    "    min_val = q25 - (iqr*threshold)\n",
    "    max_val = q75 + (iqr*threshold)\n",
    "\n",
    "    outliers = df2015[(df2015[col] > max_val) | (df2015[col] < min_val)]\n",
    "    print(col)\n",
    "    print('There are {} tracts identified as outliers.'.format(outliers.shape[0]))\n",
    "\n",
    "    df2015[col] = pd.Series(winsorize(df2015[col], limits = (outliers.shape[0]/df2015[col].shape[0])))\n",
    "\n",
    "for col in df_diff.columns:\n",
    "    threshold = 3\n",
    "    q75, q25 = np.percentile(df_diff[col], [75,25])\n",
    "    iqr = q75-q25\n",
    "    min_val = q25 - (iqr*threshold)\n",
    "    max_val = q75 + (iqr*threshold)\n",
    "\n",
    "    outliers = df_diff[(df_diff[col] > max_val) | (df_diff[col] < min_val)]\n",
    "    print(col)\n",
    "    print('There are {} tracts identified as outliers.'.format(outliers.shape[0]))\n",
    "\n",
    "    df_diff[col] = pd.Series(winsorize(df_diff[col], limits = (outliers.shape[0]/df_diff[col].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null values\n",
    "\n",
    "df2015.dropna(inplace=True)\n",
    "df_diff.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create y-variable as categorical poverty change, X variable as all 2015 statistics\n",
    "#Perform train test split; also create \"small\" dataset for faster computation during testing\n",
    "#Create scaled dataset for use when necessary\n",
    "\n",
    "y = df2015['Poverty_Change']\n",
    "\n",
    "X = df2015.drop(['State', 'County', 'Poverty_Change', 'Poverty_Change_val', 'Poverty_Change_2c'], axis=1)\n",
    "\n",
    "# df_small = df2015.sample(frac=0.1)\n",
    "# y_small = df_small['Poverty_Change_2c']\n",
    "# X_small = df_small.drop(['State', 'County', 'Poverty_Change', 'Poverty_Change_val', 'Poverty_Change_2c'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "# X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_small,y_small,test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)\n",
    "# X_train_s_scale = pd.DataFrame(scaler.fit_transform(X_train_s), columns = X_train_s.columns)\n",
    "# X_test_s_scale = pd.DataFrame(scaler.fit_transform(X_test_s), columns = X_test_s.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE()\n",
    "\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform dummy classification for baseline\n",
    "\n",
    "dummy = DummyClassifier(strategy = 'most_frequent')\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "print(dummy.score(X_train, y_train))\n",
    "print(dummy.score(X_test, y_test))\n",
    "\n",
    "plot_confusion_matrix(dummy, X_test, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logit = LogisticRegression(max_iter=10000)\n",
    "\n",
    "logit.fit(X_train_scale, y_train)\n",
    "print(logit.score(X_train_scale, y_train))\n",
    "print(logit.score(X_test_scale, y_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_confusion_matrix(logit, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = logit.predict_proba(X_test)\n",
    "# y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_cat = np.where(y_pred[:,1] >= 0.5, 1,0)\n",
    "# y_pred_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logit_smote = LogisticRegression(max_iter=10000)\n",
    "\n",
    "logit_smote.fit(X_train_res, y_train_res)\n",
    "print(logit_smote.score(X_train_res, y_train_res))\n",
    "print(logit_smote.score(X_train_scale, y_train))\n",
    "print(logit_smote.score(X_test_scale, y_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_confusion_matrix(logit_smote, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logit_smote_saga = LogisticRegression(max_iter=10000, solver='saga')\n",
    "\n",
    "logit_smote_saga.fit(X_train_res, y_train_res)\n",
    "print(logit_smote_saga.score(X_train_res, y_train_res))\n",
    "print(logit_smote_saga.score(X_train_scale, y_train))\n",
    "print(logit_smote_saga.score(X_test_scale, y_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_confusion_matrix(logit_smote_saga, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logit_smote_saga2 = LogisticRegression(max_iter=10000, solver='saga', penalty='l1')\n",
    "\n",
    "logit_smote_saga2.fit(X_train_res, y_train_res)\n",
    "print(logit_smote_saga2.score(X_train_res, y_train_res))\n",
    "print(logit_smote_saga2.score(X_train_scale, y_train))\n",
    "print(logit_smote_saga2.score(X_test_scale, y_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_confusion_matrix(logit_smote_saga2, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logit_smote_saga3 = LogisticRegression(max_iter=10000, solver='saga', penalty='elasticnet', l1_ratio=0.5)\n",
    "\n",
    "logit_smote_saga3.fit(X_train_res, y_train_res)\n",
    "print(logit_smote_saga3.score(X_train_res, y_train_res))\n",
    "print(logit_smote_saga3.score(X_train_scale, y_train))\n",
    "print(logit_smote_saga3.score(X_test_scale, y_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_confusion_matrix(logit_smote_saga3, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logit_smote_saga_ovr = LogisticRegression(max_iter=10000, solver='saga', multi_class='ovr')\n",
    "\n",
    "logit_smote_saga_ovr.fit(X_train_res, y_train_res)\n",
    "print(logit_smote_saga_ovr.score(X_train_res, y_train_res))\n",
    "print(logit_smote_saga_ovr.score(X_train_scale, y_train))\n",
    "print(logit_smote_saga_ovr.score(X_test_scale, y_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_confusion_matrix(logit_smote_saga_ovr, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logit_smote_saga_ovr2 = LogisticRegression(max_iter=10000, solver='saga', multi_class='ovr', penalty='l1')\n",
    "\n",
    "logit_smote_saga_ovr2.fit(X_train_res, y_train_res)\n",
    "print(logit_smote_saga_ovr2.score(X_train_res, y_train_res))\n",
    "print(logit_smote_saga_ovr2.score(X_train_scale, y_train))\n",
    "print(logit_smote_saga_ovr2.score(X_test_scale, y_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_confusion_matrix(logit_smote_saga_ovr2, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logit_smote_saga_ovr3 = LogisticRegression(max_iter=10000, solver='saga', multi_class='ovr', penalty='elasticnet', l1_ratio=0.5)\n",
    "\n",
    "logit_smote_saga_ovr3.fit(X_train_res, y_train_res)\n",
    "print(logit_smote_saga_ovr3.score(X_train_res, y_train_res))\n",
    "print(logit_smote_saga_ovr3.score(X_train_scale, y_train))\n",
    "print(logit_smote_saga_ovr3.score(X_test_scale, y_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plot_confusion_matrix(logit_smote_saga_ovr3, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':stats.uniform(0.1, 100)}\n",
    "parameters_elasticnet = {'C':stats.uniform(0.1, 0.9), 'l1_ratio':stats.uniform(0.1, 0.9)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "grid_smote_saga = RandomizedSearchCV(logit_smote_saga, parameters, cv=3, n_iter=30, n_jobs=2)\n",
    "grid_smote_saga.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(grid_smote_saga.best_score_)\n",
    "print(grid_smote_saga.best_params_)\n",
    "plot_confusion_matrix(grid_smote_saga.best_estimator_, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "grid_smote_saga2 = RandomizedSearchCV(logit_smote_saga2, parameters, cv=3, n_iter=30, n_jobs=2)\n",
    "grid_smote_saga2.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(grid_smote_saga2.best_score_)\n",
    "print(grid_smote_saga2.best_params_)\n",
    "plot_confusion_matrix(grid_smote_saga2.best_estimator_, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "grid_smote_saga_ovr = RandomizedSearchCV(logit_smote_saga_ovr, parameters, cv=3, n_iter=30, n_jobs=2)\n",
    "grid_smote_saga_ovr.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(grid_smote_saga_ovr.best_score_)\n",
    "print(grid_smote_saga_ovr.best_params_)\n",
    "plot_confusion_matrix(grid_smote_saga_ovr.best_estimator_, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "grid_smote_saga_ovr2 = RandomizedSearchCV(logit_smote_saga_ovr2, parameters, cv=3, n_iter=30, n_jobs=2)\n",
    "grid_smote_saga_ovr2.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(grid_smote_saga_ovr2.best_score_)\n",
    "print(grid_smote_saga_ovr2.best_params_)\n",
    "plot_confusion_matrix(grid_smote_saga_ovr2.best_estimator_, X_test_scale, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# boost_c = GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000, max_depth=2, subsample=0.5, max_features='sqrt')\n",
    "\n",
    "# boost_c.fit(X_train, y_train)\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# print(boost_c.score(X_train, y_train))\n",
    "# print(boost_c.score(X_test, y_test))\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(boost_c, X_test, y_test, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# boost_c_smote = GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000, max_depth=3, max_features='sqrt')\n",
    "\n",
    "# boost_c_smote.fit(X_train_res, y_train_res)\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# print(boost_c_smote.score(X_train, y_train))\n",
    "# print(boost_c_smote.score(X_test, y_test))\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(boost_c_smote, X_test, y_test, cmap='Blues')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
